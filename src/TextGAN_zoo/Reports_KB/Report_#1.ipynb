{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Report #1 - 15.04.2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Previous research\n",
    "\n",
    "### GANs\n",
    "GANs where introduced in 2014 (Generative Adversarial Networks - Godfellow et al. 2014) and since then, a lot of research around image GANs has been done (for example deep convolutionl GAN) and it's now possible to generate very high quality images. However the same can't be said for text GANs (usually based on LSTM blocks) which received less attention, probably because of the inherent difficulties in training GANs (mode collapse, non-convergence and instability) and the availibility of other architectures like the trasnformer. \n",
    "\n",
    "\n",
    "### The Transformer\n",
    "In the past years transformers (\"Attention is all you need - Vaswani et al, 2017\") have shown excellent performance in NLP tasks, in particular language trasnlation, thanks to their multi-head-self-attention component. They are now the default choice for NLP tasks and present in state of the art text nerual networks like BERT (\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" - Devlin et al. 2018) and the GPT family (OpenaAI).\n",
    "\n",
    "\n",
    "### GAN_EVO\n",
    "So far the GAN_EVO project focused on implementing evolutionary features in a population of image GANs to tackle common issues that can arise during training, in particular it improve the diversity of their output and avoid mode collapse. \n",
    "A translation to PyTorch of the Fr√©chet Inception Distance metric is also part of the repository.\n",
    "\n",
    "### ScratchGAN\n",
    "ScratchGAN (\"Training Language GANs from Scratch\", d'Automne et al. 2020) is the first text GAN architectures that doesn't require any MLE pretraining, and provide a set of useful tricks to avoid common issues during training. In particular the authors recommand to use large mini-bacthes, dense rewards (at word-level), big datasets and reinforcement learning for training. As expected for text GANs, the architecture is based on LSTM blocks.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Research and results\n",
    "\n",
    "### Research project\n",
    "The goal of this research project is to show that high quality text generation through text GANs is possible and that it should be thus considered as a potential threat to public information. \n",
    "\n",
    "### Ongoing research\n",
    "To achieve state of the art quality in text generation through GAN, we are investigating the possibility of replacing the LSTM module in an existing text GAN with a transformer layer. \n",
    "\n",
    "We discovered and decided to use the repository TextGAN-PyTorch (https://github.com/williamSYSU/TextGAN-PyTorch) as our starting point for experimentation with text GANs. Once understood its structure, we picked the DP-GAN (Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text, Xu et al. 2018) implementation as our candidate for the transformer layer implementation, this experiment take the name of SADPGAN where the \"SA\" stands for Self Attention. \n",
    "\n",
    "\n",
    "The main challenge consisted in harmonizing the newly introduced self attention layer with the rest of the repository: forward functions for trasnformer and lstm take different parameters and thus a general refactoring would be necessary. Since we're actively experimenting and code is quickly changing, as a workaround we created temporary a copy adapted for self attention for each part of the repository. Below are reported the structure and forward function of the LSTM-based generator and respectively of our version implementing a trasformer layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTMGenerator, orginal code\n",
    "def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n",
    "    ...\n",
    "    self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "    self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    ...\n",
    "\n",
    "def forward(self, inp, hidden, need_hidden=False):    \n",
    "    emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n",
    "    if len(inp.size()) == 1:\n",
    "        emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
    "    out, hidden = self.lstm(emb, hidden)  # out: batch_size * seq_len * hidden_dim\n",
    "    out = out.contiguous().view(-1, self.hidden_dim)\n",
    "    out = self.lstm2out(out)  # (batch_size * seq_len) * vocab_size\n",
    "    pred = self.softmax(out)\n",
    "    if need_hidden:\n",
    "        return pred, hidden\n",
    "    else:\n",
    "        return pred\n",
    "\n",
    "#TransformerGenerator, our experiment\n",
    "def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, num_heads=4, nlayers=4, dropout=0.5, gpu=False):\n",
    "    ...\n",
    "    self.encoder = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "    self.pos_encoder = PositionalEncoding(embedding_dim, dropout)\n",
    "    encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
    "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "    self.decoder = nn.Linear(embedding_dim, vocab_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=-1)  \n",
    "    ...  \n",
    "\n",
    "def forward(self, inp, src_mask=None):\n",
    "        src = self.encoder(inp) * math.sqrt(self.embedding_dim)\n",
    "        if len(inp.size()) == 1:\n",
    "            src = src.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
    "        src = self.pos_encoder(src)\n",
    "        #output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * len) * hidden_dim\n",
    "        output = self.decoder(output)\n",
    "        pred = self.softmax(output)\n",
    "        return pred"
   ]
  },
  {
   "source": [
    "As it can be seen, we hevely reley on the original code of the LSTMGenerator, but at the same time we replace the LSTM block with the encoder layer of the Transformer, without forgetting the positional encoders. For the sake of simplicity, in this first attempt we deliberatly omitted the src_mask in the forward function. These modifications were inspired by the PyTorch tutorial \"SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT\" (https://pytorch.org/tutorials/beginner/transformer_tutorial.html), and by the architecture of TransGAN (Two Transformers Can Make One Strong GAN, Jiang et al. 2021), a very recent GAN replacing convolutional layers with attention ones to generate images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| <img src=\"transformer.png\"/> | <img src=\"transgan.png\"/> |\n",
    "|------------------------------|---------------------------|\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Results\n",
    "We trained both DPGAN and SADPGAN on the same machine, 120 epoch of MLE pretraining and 200 adversarial rounds with neural networks of similar size. Unfortunately we observed that no learning is occuring with the actual implementation of SADPGAN: both for quality and diversity, the network doesn't improve at all during pretraining. Without surprise the abscence of improvement persisted also during adversarial training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| <img src=\"G_MLE_NLL_div.png\"/> | <img src=\"G_MLE_NLL_gen.png\"/> |\n",
    "|--------------------------------|--------------------------------|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Conclusions and future work\n",
    "Cleary the actual implementation of SADPGAN is not delivering the expected results. However we reamain confident in our idea of replacing the LSTM block with a transformer layer, and believe that the total abscence of learning is due to poor implementation rather than a conceptual mistake. \n",
    "Our priority is thus debug the code and understand what went wrong with the training of SADPGAN. Our first lead is the missing implementation of masking in the forward function: probably it is more important than first tought. Second possibility is wrong implementation of the characteristic word-level feedback of DPGAN in our version  \n",
    "\n",
    "We consider ScratchGAN as the sota for text GANs, but unfortunately its implementation is only available in TensorFlow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Appendix\n",
    "\n",
    "### A. Parameters for DPGAN and SADPGAN training\n",
    "\n",
    "Here is the list of full parameters for both experiments:\n",
    "\n",
    "| DPGAN > training arguments: >>> if_test: 0 >>> run_model: dpgan >>> k_label: 2 >>> dataset: oracle >>> model_type: vanilla >>> loss_type: rsgan >>> mu_type: ragan >>> eval_type: Ra >>> d_type: Ra >>> if_real_data: 0 >>> sa: False >>> cuda: 1 >>> device: 0 >>> devices: 0 >>> shuffle: 0 >>> gen_init: normal >>> dis_init: uniform >>> n_parent: 1 >>> eval_b_num: 8 >>> lambda_fq: 1.0 >>> lambda_fd: 0.0 >>> d_out_mean: True >>> freeze_dis: False >>> freeze_clas: False >>> use_all_real_fake: False >>> use_population: False >>> samples_num: 1000 >>> vocab_size: 5000 >>> mle_epoch: 120 >>> clas_pre_epoch: 10 >>> adv_epoch: 200 >>> inter_epoch: 15 >>> batch_size: 64 >>> max_seq_len: 20 >>> start_letter: 1 >>> padding_idx: 0 >>> gen_lr: 0.01 >>> gen_adv_lr: 0.0001 >>> dis_lr: 0.01 >>> clip_norm: 5.0 >>> pre_log_step: 10 >>> adv_log_step: 1 >>> train_data: dataset/oracle.txt >>> test_data: dataset/testdata/oracle_test.txt >>> temp_adpt: exp >>> evo_temp_step: 1 >>> temperature: 1 >>> ora_pretrain: 1 >>> gen_pretrain: 0 >>> dis_pretrain: 0 >>> adv_g_step: 1 >>> rollout_num: 16 >>> gen_embed_dim: 32 >>> gen_hidden_dim: 32 >>> goal_size: 16 >>> step_size: 4 >>> mem_slots: 1 >>> num_heads: 4 >>> head_size: 64 >>> gen_nlayers: 3 >>> gen_num_heads: 4 >>> dropout: 0.5 >>> d_step: 5 >>> d_epoch: 3 >>> adv_d_step: 4 >>> adv_d_epoch: 2 >>> dis_embed_dim: 64 >>> dis_hidden_dim: 64 >>> num_rep: 64 >>> dis_nlayers: 2 >>> dis_num_heads: 4 >>> use_nll_oracle: 1 >>> use_nll_gen: 1 >>> use_nll_div: 1 >>> use_bleu: 1 >>> use_self_bleu: 0 >>> use_clas_acc: True >>> use_ppl: 0 >>> log_file: log/log_0413_0931_30.txt >>> save_root: save/20210413/oracle/dpgan_vanilla_dt-Ra_lt-rsgan_mt-ra_et-Ra_sl20_temp1_lfd0.0_T0413_0931_30/ >>> signal_file: run_signal.txt >>> tips: DPGAN experiments | SADPGAN > training arguments: >>> if_test: 0 >>> run_model: sa_dpgan >>> k_label: 2 >>> dataset: oracle >>> model_type: pineapple >>> loss_type: rsgan >>> mu_type: ragan >>> eval_type: Ra >>> d_type: Ra >>> if_real_data: 0 >>> sa: 1 >>> cuda: 1 >>> device: 0 >>> devices: 0 >>> shuffle: 0 >>> gen_init: normal >>> dis_init: uniform >>> n_parent: 1 >>> eval_b_num: 8 >>> lambda_fq: 1.0 >>> lambda_fd: 0.0 >>> d_out_mean: True >>> freeze_dis: False >>> freeze_clas: False >>> use_all_real_fake: False >>> use_population: False >>> samples_num: 10000 >>> vocab_size: 5000 >>> mle_epoch: 120 >>> clas_pre_epoch: 10 >>> adv_epoch: 200 >>> inter_epoch: 15 >>> batch_size: 64 >>> max_seq_len: 20 >>> start_letter: 1 >>> padding_idx: 0 >>> gen_lr: 0.01 >>> gen_adv_lr: 0.0001 >>> dis_lr: 0.01 >>> clip_norm: 5.0 >>> pre_log_step: 10 >>> adv_log_step: 1 >>> train_data: dataset/oracle.txt >>> test_data: dataset/testdata/oracle_test.txt >>> temp_adpt: exp >>> evo_temp_step: 1 >>> temperature: 1 >>> ora_pretrain: 0 >>> gen_pretrain: 0 >>> dis_pretrain: 0 >>> adv_g_step: 1 >>> rollout_num: 16 >>> gen_embed_dim: 32 >>> gen_hidden_dim: 32 >>> goal_size: 16 >>> step_size: 4 >>> mem_slots: 1 >>> num_heads: 4 >>> head_size: 64 >>> gen_nlayers: 4 >>> gen_num_heads: 4 >>> dropout: 0.5 >>> d_step: 5 >>> d_epoch: 3 >>> adv_d_step: 4 >>> adv_d_epoch: 2 >>> dis_embed_dim: 64 >>> dis_hidden_dim: 64 >>> num_rep: 64 >>> dis_nlayers: 4 >>> dis_num_heads: 4 >>> use_nll_oracle: 1 >>> use_nll_gen: 1 >>> use_nll_div: 1 >>> use_bleu: 1 >>> use_self_bleu: 0 >>> use_clas_acc: True >>> use_ppl: 0 >>> log_file: log/log_0413_0935_02.txt >>> save_root: save/20210413/oracle/sa_dpgan_pineapple_dt-Ra_lt-rsgan_mt-ra_et-Ra_sl20_temp1_lfd0.0_T0413_0935_02/ >>> signal_file: run_signal.txt >>> tips: DPGAN experiments |   |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}